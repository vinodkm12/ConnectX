{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install kaggle-environments","metadata":{}},{"cell_type":"code","source":"# 1. Enable Internet in the Kernel (Settings side pane)\n\n# 2. Curl cache may need purged if v0.1.6 cannot be found (uncomment if needed). \n# !curl -X PURGE https://pypi.org/simple/kaggle-environments\n\n# ConnectX environment was defined in v0.1.6\n!pip install 'kaggle-environments>=0.1.6'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-25T09:51:13.236421Z","iopub.execute_input":"2021-12-25T09:51:13.23689Z","iopub.status.idle":"2021-12-25T09:51:22.855583Z","shell.execute_reply.started":"2021-12-25T09:51:13.236817Z","shell.execute_reply":"2021-12-25T09:51:22.854301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create ConnectX Environment","metadata":{}},{"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\n\nenv = make(\"connectx\", debug=True)\nenv.render()\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-12-25T09:51:22.857725Z","iopub.execute_input":"2021-12-25T09:51:22.85824Z","iopub.status.idle":"2021-12-25T09:51:23.133201Z","shell.execute_reply.started":"2021-12-25T09:51:22.858185Z","shell.execute_reply":"2021-12-25T09:51:23.132562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create an Agent\n\nTo create the submission, an agent function should be fully encapsulated (no external dependencies).  \n\nWhen your agent is being evaluated against others, it will not have access to the Kaggle docker image.  Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy, pytorch (1.3.1, cpu only), and more may be added later.\n\n","metadata":{}},{"cell_type":"code","source":"# This agent random chooses a non-empty column.\ndef my_agent(observation, configuration):\n    WINNING_P1 = 100000\n    WINNING_P2 = -100000\n    \n    #Tests the number of traps set in a particular column and row\n    def eval_entry(player, row_num, col_num, observation, configuration):\n        return_score = 0\n        #Get token in current row and column\n        def get(row_num, col_num):\n            if row_num < 0 or row_num >= configuration.rows or col_num < 0 or col_num >= configuration.columns:\n                return -1\n            else:\n                return observation.board[row_num * configuration.columns + col_num]\n        #Test whether a current token is in a X in a row configuration\n        #Delta stores the directions of the current X in a row we are testing (up down diagonal)\n        def isInX(row_num, col_num, delta):\n            numRight = 0\n            for i in range(1, configuration.inarow + 1):\n                if get(row_num + i*delta[0], col_num + i*delta[1]) == player:\n                    numRight = numRight + 1\n                else:\n                    break\n            numLeft = 0\n            for i in range(1, configuration.inarow + 1):\n                if get(row_num - i*delta[0], col_num - i*delta[1]) == player:\n                    numLeft = numLeft + 1\n                else:\n                    break\n            if numLeft + numRight + 1 >= configuration.inarow:\n                return True\n            else:\n                return False\n                \n        def set_val(row_num, col_num, target):\n            if isInX(row_num, col_num, [1,0]):\n                return target\n            elif isInX(row_num, col_num, [0,1]):\n                return target\n            elif isInX(row_num, col_num, [1,1]):\n                return target\n            elif isInX(row_num, col_num, [-1,1]):\n                return target\n            return 0\n\n        if get(row_num, col_num) == 0:\n            return_score = set_val(row_num, col_num, 1)\n        elif get(row_num, col_num) == player:\n            return_score = set_val(row_num, col_num, WINNING_P1)\n        return return_score\n    \n    \n    def eval_board(observation, configuration):\n        ENDGAME_CONTROL = int(0.5*configuration.rows)\n        #Give a point bonus to having the first trap in a column\n        FIRST_BONUS = int(0.4*configuration.rows)\n        score =  0\n        def get(row_num, col_num):\n            if row_num < 0 or row_num >= configuration.rows or col_num < 0 or col_num >= configuration.columns:\n                return -1\n            else:\n                return observation.board[row_num * configuration.columns + col_num]\n\n        #Iterate through al columns to evaluate each\n        for col in range(configuration.columns):\n            numZeros = 0\n            for row in range(configuration.rows):\n                if get(row, col) == 0:\n                    numZeros = numZeros + 1\n                else:\n                    break\n            evenControl = 0\n            oddControl = 0\n            for row in range(numZeros):\n                if row % 2 == 0:\n                    evenControl = evenControl + eval_entry(1, row, col, observation, configuration) - eval_entry(2, row, col, observation, configuration)\n                elif row % 2 == 1:\n                    oddControl=oddControl + eval_entry(1, row, col, observation, configuration) - eval_entry(2, row, col, observation, configuration)\n            \n            #Test for even numbered square control and odd numbered square control\n            #If agent has both, we give them a bonus for endgame control in this column\n            score = score + evenControl + oddControl\n            if numZeros >= 2:\n                score = score + (FIRST_BONUS - 1) * (eval_entry(1, numZeros-2, col, observation, configuration) - eval_entry(2, numZeros-2, col, observation, configuration))\n            if evenControl > 0 and oddControl > 0:\n                score = score + ENDGAME_CONTROL\n            if evenControl < 0 and oddControl < 0:\n                score = score - ENDGAME_CONTROL\n\n            #Test every single row for instant wins and losses\n            #We avoid instantly losing at all costs\n            #If an instant win or loss exists, return score right away (saves massive computation time)\n            for row in range(numZeros, configuration.rows):\n                if get(row,col) == 1:\n                    score = score + eval_entry(1, row, col, observation, configuration)\n                    if score >= 50000:\n                        return score\n                elif get(row,col) == 2:\n                    score = score - eval_entry(2, row, col, observation, configuration)\n                    if score <= -50000:\n                        return score\n        return score    \n\n    #Place a token and then take back right away\n    def place(observation, configuration, col, player):\n        #Get the specified token in the board\n        def get(row_num, col_num):\n            if row_num < 0 or row_num >= configuration.rows or col_num < 0 or col_num >= configuration.columns:\n                return -1\n            else:\n                return observation.board[row_num * configuration.columns + col_num]\n        \n        #Update the observation board with the player data\n        for row in range(configuration.rows):\n            if get(row, col) == 0:\n                if row == configuration.rows - 1 or get(row+1, col) == 1 or get(row+1, col) == 2:\n                    observation.board[row*configuration.columns + col] = player\n                    return row\n        return -1\n    \n    #Maximizing agent in alpha-beta pruned minimax\n    def maxi(observation, configuration, depth, alpha, beta):\n        #If column isn't full, add to current list of moves\n        moves = [c for c in range(configuration.columns) if observation.board[c] == 0]\n        midPt = len(moves)//2\n        #We want to bias to placing moves in the center, so we reorder the list\n        moves = moves[midPt:] + moves[0:midPt]\n        maxScore = None\n        bestMove = None\n        #If we are at the final depth in the tree, evaluate right away\n        if depth == 0: \n            for move in moves:\n                #Place token and take it back for the purpose of evaluation\n                row_place = place(observation, configuration, move, 1)\n                score = eval_board(observation, configuration)\n                if maxScore is None or score > maxScore: \n                    maxScore = score;\n                    bestMove = move\n                #If pruning can be done, prune it\n                #Otherwise return move and end right away\n                if maxScore >= beta:\n                    observation.board[row_place * configuration.columns + move] = 0\n                    return [bestMove, maxScore]\n                if maxScore > alpha:\n                    alpha = maxScore\n                \n                \n                observation.board[row_place * configuration.columns + move] = 0\n\n        else:\n            #Pair stores the data from the previous depth of either maxi or mini\n            for move in moves:\n                row_place = place(observation, configuration, move, 1)\n                crnt_eval = eval_board(observation, configuration)\n                if crnt_eval >= WINNING_P1/2 or  crnt_eval <= WINNING_P2/2:\n                    pair = None\n                else:\n                    pair = mini(observation, configuration, depth-1, alpha, beta)\n                if pair is not None and pair[0] is not None:\n                    score = pair[1]\n                    if maxScore is None or score > maxScore:\n                        maxScore = score\n                        bestMove = move\n                    \n                    if maxScore >= beta:\n                        observation.board[row_place * configuration.columns + move] = 0\n                        return [bestMove, maxScore]\n                    if maxScore > alpha:\n                        alpha = maxScore;      \n                else:\n                    score = crnt_eval\n                    #Update maximum score and test for best move\n                    if maxScore is None or score > maxScore:\n                        maxScore = score\n                        bestMove = move\n                    #If move can be pruned, prune it\n                    #Otherwise, end evaluation right away\n                    if maxScore >= beta:\n                        observation.board[row_place * configuration.columns + move] = 0\n                        return [bestMove, maxScore]\n                    if maxScore > alpha:\n                        alpha = maxScore;\n                    \n                        \n                observation.board[row_place * configuration.columns + move] = 0\n        return [bestMove, maxScore]\n    \n    #Mini player in alpha-beta pruned decision tree engine\n    def mini(observation, configuration, depth, alpha, beta):\n        moves = [c for c in range(configuration.columns) if observation.board[c] == 0]\n        midPt = len(moves)//2\n        #Bias to placing moves in the center\n        moves = moves[midPt:] + moves[0:midPt]\n        #Initialize best move and minimum score variables\n        minScore = None\n        bestMove = None\n        if depth == 0:\n            for move in moves:\n                row_place = place(observation, configuration, move, 2)\n                score = eval_board(observation, configuration)\n                #Update minimum score and keep going\n                if minScore is None or score < minScore:\n                    minScore = score;\n                    bestMove = move\n                #If the column can be pruned away, prune it\n                #Otherwise, end evaluation right away\n                if minScore <= alpha:\n                    observation.board[row_place * configuration.columns + move] = 0\n                    return [bestMove, minScore]\n                if minScore < beta:\n                    beta = minScore;\n            \n                observation.board[row_place * configuration.columns + move] = 0\n        else:\n            for move in moves:\n                row_place = place(observation, configuration, move, 2)\n                crnt_eval = eval_board(observation, configuration)\n                if crnt_eval >= WINNING_P1/2 or crnt_eval <= WINNING_P2/2:\n                    pair = None\n                else:\n                    pair = maxi(observation, configuration, depth-1, alpha, beta)\n                if pair is not None and pair[0] is not None: \n                    score = pair[1]\n                    if minScore is None or score < minScore:\n                        minScore = score\n                        bestMove = move\n                    \n                    if minScore <= alpha:\n                        observation.board[row_place * configuration.columns + move] = 0\n                        return [bestMove, minScore]\n                    if minScore < beta:\n                        beta = minScore\n                    \n                else:\n                    score = crnt_eval\n                    if minScore is None or score < minScore:\n                        minScore = score\n                        bestMove = move\n                    \n                    if minScore <= alpha:\n                        observation.board[row_place * configuration.columns + move] = 0\n                        return [bestMove, minScore]\n                    if minScore < beta:\n                        beta = minScore\n                    \n                    \n                observation.board[row_place * configuration.columns + move] = 0\n        return [bestMove, minScore]\n\n    if observation.mark == 1:\n        ans = maxi(observation, configuration, 4, -300000, 300000)\n    else:\n        ans = mini(observation, configuration, 4, -300000, 300000)\n    return ans[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:51:23.134249Z","iopub.execute_input":"2021-12-25T09:51:23.134586Z","iopub.status.idle":"2021-12-25T09:51:23.178274Z","shell.execute_reply.started":"2021-12-25T09:51:23.134548Z","shell.execute_reply":"2021-12-25T09:51:23.17722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test your Agent","metadata":{}},{"cell_type":"code","source":"env.reset()\n# Play as the first agent against default \"random\" agent.\nenv.run([ my_agent, my_agent])\nenv.render(mode=\"ipython\", width=500, height=450)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:51:23.179551Z","iopub.execute_input":"2021-12-25T09:51:23.179927Z","iopub.status.idle":"2021-12-25T09:51:52.058453Z","shell.execute_reply.started":"2021-12-25T09:51:23.179772Z","shell.execute_reply":"2021-12-25T09:51:52.057209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Debug/Train your Agent","metadata":{}},{"cell_type":"code","source":"# Play as first position against random agent.\ntrainer = env.train([None, \"random\"])\n\nobservation = trainer.reset()\nwhile not env.done:\n    my_action = my_agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\n    # env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nenv.render()","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:51:52.05999Z","iopub.execute_input":"2021-12-25T09:51:52.060205Z","iopub.status.idle":"2021-12-25T09:51:56.384179Z","shell.execute_reply.started":"2021-12-25T09:51:52.060172Z","shell.execute_reply":"2021-12-25T09:51:56.383323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate your Agent","metadata":{}},{"cell_type":"code","source":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) / float(len(rewards))\n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:51:56.385633Z","iopub.execute_input":"2021-12-25T09:51:56.386049Z","iopub.status.idle":"2021-12-25T09:52:36.611612Z","shell.execute_reply.started":"2021-12-25T09:51:56.385841Z","shell.execute_reply":"2021-12-25T09:52:36.609978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Play your Agent\nClick on any column to place a checker there (\"manually select action\").","metadata":{}},{"cell_type":"code","source":"# \"None\" represents which agent you'll manually play as (first or second player).\nenv.play([None, \"negamax\"], width=500, height=450)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:52:36.612805Z","iopub.status.idle":"2021-12-25T09:52:36.613476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Write Submission File\n\n","metadata":{}},{"cell_type":"code","source":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(my_agent, \"submission.py\")","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:52:42.522684Z","iopub.execute_input":"2021-12-25T09:52:42.523172Z","iopub.status.idle":"2021-12-25T09:52:42.540407Z","shell.execute_reply.started":"2021-12-25T09:52:42.523109Z","shell.execute_reply":"2021-12-25T09:52:42.539355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate Submission\nPlay your submission against itself.  This is the first episode the competition will run to weed out erroneous agents.\n\nWhy validate? This roughly verifies that your submission is fully encapsulated and can be run remotely.","metadata":{}},{"cell_type":"code","source":"# Note: Stdout replacement is a temporary workaround.\nimport sys\nout = sys.stdout\nsubmission = utils.read_file(\"/kaggle/working/submission.py\")\nagent = utils.get_last_callable(submission)\nsys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:52:36.616471Z","iopub.status.idle":"2021-12-25T09:52:36.617118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit to Competition\n\n1. Commit this kernel.\n2. View the commited version.\n3. Go to \"Data\" section and find submission.py file.\n4. Click \"Submit to Competition\"\n5. Go to [My Submissions](https://kaggle.com/c/connectx/submissions) to view your score and episodes being played.","metadata":{}}]}